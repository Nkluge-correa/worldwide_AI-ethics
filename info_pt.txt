Desde o final do nosso último *“inverno da IA",* 1987 – 1993, a pesquisa em IA e sua indústria têm visto um crescimento maciço, seja em tecnologias desenvolvidas, investimentos, atenção da mídia ou novas tarefas que sistemas autônomos são hoje, capazes de realizar. Se olharmos para a história das submissões no ArXiv ([entre 2009 e 2021](https://arxiv.org/about/reports/submission_category_by_year)), um repositório de preprints e publicações eletrônicas de acesso aberto, a partir de 2018, **trabalhos relacionados à Ciência da Computação têm sido o tipo mais comum de material submetido.** 
Corrêa, N.K., Galvão, C., Pinto, E.S., William, J., Del Pino, C., Barbosa, C., Massmann, D.F., Mambrini, R., Galvão, L., Terem, E. (2022). Worldwide AI Ethics: a review of 200 guidelines and recommendations for AI governance. *AI Robotics Ethics Society (PUCRS).* [https://doi.org/10.48550/arXiv.2206.11922](https://doi.org/10.48550/arXiv.2206.11922).
Olhando para a distribuição entre regiões do mundo (agregadas por continente), vemos que a maior parte dos documentos produzidos vem da **Europa, América do Norte e Ásia**, enquanto regiões como **América do Sul, África e Oceania representam menos de 4,5% de toda a nossa amostra**. Se não fosse pela **participação significativa de Organizações Intergovernamentais,** como a OTAN, NU, UNESCO, **que representam 6% de nossa amostra (13 documentos),** outras regiões/países do mundo estariam ainda mais sub representadas.
77% do tamanho total de nossa amostra é composta por 13 países, **Estados Unidos da América, Reino Unido, Alemanha, Canadá, China, Japão, França, Finlândia, Holanda, Suíça, Bélgica, Brasil e Coréia do Sul,** enquanto uma miríade de **24 países (12,5%) representam o restante de nossa amostra**, juntamente com organizações intergovernamentais, como a União Europeia (9 = 4,5%) e as NU (6 = 3%).
Ao remover documentos com autores não especificados, contamos um total de **561 autores homens (66,6%)** e **281 autoras mulheres (33,3%).** A predominância do gênero masculino é **uma tendência que pode ser encontrada em praticamente todas as regiões e países do mundo, independentemente do tipo de instituição.**
O gênero dos autores foi determinado pela busca de seus nomes e imagens de perfil em diferentes tipos de plataformas (e.g.,, *LinkedIn, Researchgate, sites universitários, sites pessoais, etc.*) através de motores de busca. 
Com respeito ao ano de publicação dos documentos de nossa amostra, **vemos que a maioria dos documentos (129 = 64,5%) foram publicados entre os anos 2017 e 2019.** O que podemos chamar de *"boom da ética da IA"* trata-se da **produção significativa de documentos no ano 2018**, o que representa **30,5% (61)** de toda a nossa amostra.
**Nota:** documentos com datas de publicação **não especificadas (27 = 13,5%)** também são bastante **prevalentes** em nossa amostra.
Os **cinco principais princípios** defendidos nos documentos de nossa amostra são semelhantes aos resultados obtidos por Jobin et al. ([2019](https://www.nature.com/articles/s42256-019-0088-2)) e Hagendorff ([2020](https://link.springer.com/article/10.1007/s11023-020-09517-8)), **com o acréscimo de Confiabilidade/Segurança/Confiança/Fiabilidade (78%)**, que também são citados como um dos cinco principais na metanálise de Fjeld et al. ([2020](https://dash.harvard.edu/handle/1/42160420)) **(80%)**. Como cada documento apresenta sua própria passagem sobre cada princípio, se existiam, por exemplo, 134 documentos que citam o princípio de privacidade, coletamos 134 definições/recomendações diferentes envolvendo este princípio. Todos sendo acessíveis em nosso [Power BI dashboard](https://www.airespucrs.org/worldwide-ai-ethics).
Olhando para a **distribuição de princípios filtrada por continente**, **os cinco principais princípios continuam os mesmos** tanto na **América do Norte** como **Europa**. Já no **continente asático** é introduzido o princípio de **Beneficência/Não-maleficência como o 5º (74%) princípio mais citado**, colocando Responsabilidade/Responsabilização em 6º lugar (70%). **Filtrando nossos resultados por país**, não vemos **nenhuma mudança nos cinco principais princípios ** quando comparado aos **EUA** e **RU**. Entretanto, olhando **além dos cinco mais citados princípios**, nós começamos a ver **diferenças**, como **Liberdade/Autonomia/Valores Democráticos/Soberania Tecnológica (38%)** e  **Beneficência/Não-maleficência (34,4%)** sendo o **6º** e **7º princípios mais citados nos EUA**, e **Código aberto/Concorrência Justa/Cooperação (45,8%)** e **Diversidade/Inclusão/Pluralismo/Acesbilidade (41,6%)** sendo o **6º** e **7º princípios mais citados no UK**.
Ao examinar a **distribuição de princípios filtrados por tipo de instituição**, podemos chegar a vários insights. Por exemplo, olhando para nossa amostra total, percebe-se que a **maior preocupação de instituições governamentais** (em todo o mundo) e a necessidade de **sistemas transparentes (89,5%)**, **corporações privadas** defendem principalmente **Confiabilidade e Segurança (87,5%)**, enquanto que **Organizações sem fins lucrativos e ONGs** defendem principalmente a **Jusiça (88,2%)**. 
Com exceção de instituições como **IBM (5)**, **Microsoft (4)**, e **UNESCO (3)**, **a maioria das outras instituições não têm mais do que dois documentos publicados**. Observamos também que **a maior parte de nossa amostra foi produzida por instituições governamentais e corporações privadas (48%)**,  seguidas por **ONGs (17%)**, **Organizações Sem Fins Lucrativos (16%)**, e **Instituições Acadêmicas (12,5%)**. 
No entanto, **esta tendência só se segue se olharmos para a totalidade de nossa amostra**. Se olharmos para os documentos produzidos por **continentes**,  vemos que, por exemplo, na **América do Norte (69), corporações privadas (24 = 34,7%) e organizações sem fins lucrativos (18 = 26%) produziram o maior número de documentos**, seguidas de instituições governamentais (12 = 17,4%). Ao mesmo tempo, quando olhamos para **Europa**, a **tendência global é restaurada**. 
Uma análise aprofundada **segmentada por países** mostra que o **engajamento** de certos tipos de **AI stakeholders** (i.e., tipos de instituições) **difere por países.** Por exemplo, na **China (11),  a maioria dos documentos foram produzidos por instituições acadêmicas (5 = 45,4%), enquanto que na Alemanha (20), a maioria dos documentos de nossa amostra foram produzidas por corporações privadas (6 = 30%), e NGOs (4 = 20%).** Outros insights podem ser encontrados em nosso [Power BI dashboard](https://www.airespucrs.org/worldwide-ai-ethics).
Este tipo está relacionado com a natureza/conteúdo do documento, e **três categorias** foram definidas (essas categorias *não foram definidas como mutuamente exclusivas*):
**Descritivo:** Os documentos descritivos tomam o esforço de apresentar definições factuais relacionadas à IA. Estas definições servem para contextualizar "o que queremos dizer" quando falamos de IA, e como o vocabulário utilizado neste campo pode ser compreendido;
**Normativo:** documentos normativos apresentam normas, princípios éticos, recomendações e afirmações imperativas sobre como tais tecnologias devem ou não ser utilizadas/desenvolvidas;
**Prático:** documentos práticos apresentam ferramentas de desenvolvimento para implementar princípios e normas éticas, sejam elas qualitativas (e. g., pesquisas de auto-avaliação) ou quantitativas (e. g., *Algoritmos de Debiasing* para modelos de ML).
A maior parte da nossa amostra é composta de **amostras normativas (96%)**, sendo que  **um terço desta amostra também apresenta conteúdo descritivo (55,5%)**, e mais **raramente**, implementações **práticas (27%)**.
Para criar uma "** definição geral**" de cada princípio/grupo de princípios, utilizamos uma técnica de **[mineração de texto](https://en.wikipedia.org/wiki/Text_mining)** chamada de **[análise de n-gramas](https://en.wikipedia.org/wiki/N-gram)**, onde contamos a repetição sucessiva de palavras (e grupos de palavras) em cada princípio encontrado nos documentos de nossa amostra. Assim, as definições abaixo foram criadas para contemplar os temas recorrentes que encontramos. *Abaixo também apresentamos gráficos de contagem de quatro-gramas de cada princípio.*
**Responsabilidade/Responsabilização:** responsabilidade refere-se à ideia de que os desenvolvedores e implementadores de tecnologias de IA devem estar em conformidade com os órgãos reguladores, o que também significa que tais atores devem ser responsáveis por suas ações e pelos impactos causados por suas tecnologias;
**Beneficência/Não-maleficência:** beneficência e não-maleficência são conceitos que oriundos da bioética e da ética médica. Na ética da IA, esses princípios afirmam que o bem-estar humano (e a aversão ao dano) devem ser o objetivo deste tipo de tecnologia. Algumas vezes, este princípio também está ligado à ideia de Sustentabilidade, afirmando que a IA deve ser benéfica não apenas para a civilização humana, mas para nosso meio ambiente e outros seres vivos; 
**Direitos da Criança e do Adolescente:** a ideia de que os direitos das crianças e adolescentes devem ser respeitados por tecnologias que utilizam de IA. AI stakeholders devem salvaguardar, respeitar e estar cientes das fragilidades associadas aos jovens;
**Dignidade/Direitos Humanos:** este princípio se baseia na ideia de que todos os indivíduos merecem tratamento adequado, dignidade e respeito. Na ética da IA, o respeito à dignidade humana está frequentemente ligado aos direitos humanos (i.e., a Declaração Universal dos Direitos Humanos); 
**Diversidade/Inclusão/Pluralismo/Acessibilidade:** este conjunto de princípios defende a ideia de que o desenvolvimento e o uso das tecnologias de IA devem ser feitos de forma inclusiva e acessível, respeitando as diferentes formas que a entidade humana pode vir a se expressar (gênero, etnia, raça, orientação sexual, deficiências, etc.). Este grupo de princípios está fortemente ligado a outro conjunto de princípios: Justiça/Equidade/Igualdade/Não-discriminação; 
**Liberdade/Autonomia/Valores Democráticos/Soberania Tecnológica:** este conjunto de princípios defende a ideia de que a autonomia da tomada de decisão humana deve ser preservada durante as interações humano-IA, quer essa escolha seja individual ou conjunta, como a inviolabilidade dos direitos e valores democráticos, estando também ligada à auto-suficiência tecnológica das Nações/Estados;
**Educação/Formação Humana:** tais princípios defendem a ideia de que a formação e educação humana deve ser priorizada em nossos avanços tecnológicos. Tecnologias que utilizam de IA exigem um nível considerável de especialização para serem produzidas e operadas, e tal conhecimento deve ser acessível a todos. Este princípio parece estar fortemente ligado aos Direitos Trabalhistas. A grande maioria dos documentos relativos aos trabalhadores e à vida profissional apontam para a necessidade de reeducar e requalificar a força de trabalho como uma estratégia de mitigação do desemprego tecnológico;
**Centrado no Ser Humano/Alinhamento:** tais princípios defendem a ideia de que os sistemas de IA devem ser centrados e alinhados com valores humanos. Tecnologias que utilizam de IA devem ser adaptadas para se alinharem com nossos valores (e. g., design sensível a valores). Este princípio também é usado como uma categoria "coringa", muitas vezes sendo definido como um conjunto de "princípios que são valorizados pelos humanos" (e. g., liberdade, privacidade, não-discriminação, etc.);
**Propriedade Intelectual:** este princípio procura fundamentar os direitos de propriedade sobre produtos e/ou processos de conhecimento gerado por indivíduos, sejam eles tangíveis ou intangíveis;
**Justiça/Equidade/Igualdade/Não-discriminação:** este conjunto de princípios sustenta a ideia de não-discriminação e mitigação de preconceitos (sistemas de IA podem estar sujeitos a preconceitos algorítmicos discriminatórios). Aqui também se defende a ideia de que, independentemente dos diferentes atributos sensíveis que possam caracterizar um indivíduo, todos devem ser tratados "justamente";
**Direitos Trabalhistas:** Os direitos trabalhistas são direitos legais e humanos relacionados às relações de trabalho entre trabalhadores e empregadores. Na ética da IA, este princípio enfatiza que os direitos dos trabalhadores devem ser preservados, independentemente de que as relações de trabalho estejam ou não sendo mediadas por tecnologias que utilizam de IA. Uma das principais preocupações apontadas quando este princípio é apresentado é a mitigação do desemprego tecnológico (e. g., , através da Educação/Formação Humana);
**Código Aberto/Concorrência Justa/Cooperação:** este conjunto de princípios defende diferentes meios pelos quais ações conjuntas podem ser estabelecidas e cultivadas entre AI stakeholders para alcançar objetivos comuns. Também defende-se o intercâmbio livre e aberto de ativos valiosos para a IA (e. g., dados, conhecimento, direitos de patente, recursos humanos) para mitigar possíveis monopólios tecnológicos;
**Privacidade:** a ideia de privacidade pode ser definida como o direito do indivíduo de "expor-se voluntariamente, e na medida do desejado, ao mundo". Na ética da IA, este princípio sustenta o direito de uma pessoa a controlar a exposição e disponibilidade de informações pessoais quando extraídas como dados de treinamento para sistemas de IA. Este princípio também está relacionado a conceitos tais como minimização de dados, anonimato, consentimento informado e outros conceitos relacionados à proteção de dados; 
**Confiabilidade/Segurança/Confiança/Fiabilidade:** este conjunto de princípios sustenta a ideia de que as tecnologias de IA devem ser confiáveis, no sentido de que seu uso pode ser comprovadamente atestado como seguro e robusto, promovendo a confiança do usuário e uma melhor aceitação das tecnologias de IA; 
**Sustentabilidade:** este princípio pode ser entendido como uma forma de "justiça intergeracional", onde o bem-estar das gerações futuras também deve ser considerado durante o desenvolvimento da IA. Na ética da IA, sustentabilidade se refere à ideia de que o desenvolvimento de tecnologias de IA deve ser realizado com consciência de suas implicações a longo prazo, tais como custos ambientais e preservação/bem estar da vida não-humana;
**Transparência/Explicabilidade/Auditoria:** este conjunto de princípios apoia a ideia de que o uso e desenvolvimento da IA deve ser feito de forma transparente para todos stakeholders. A transparência pode estar relacionada com "a transparência de uma organização" ou "a transparência de um algoritmo". Este conjunto de princípios também está relacionado à ideia de que tais informações devem ser compreensíveis para não especialistas, e, quando necessário, sujeitas a auditoria;
**Veracidade:** este princípio sustenta a ideia de que a IA deve fornecer informações verdadeiras. Está também relacionado à ideia de que as pessoas não devem ser enganadas quando interagem com sistemas de IA. Este princípio está fortemente relacionado com a mitigação de meios automatizados de desinformação.
Este tipo está relacionado com a forma de regulamentação que o documento propõe. Para isso, foram definidas **três categorias** (estas categorias foram *definidas como mutuamente exclusivas*):
**Regulamentação Governamental:** esta categoria foi definida para abranger estritamente documentos feitos por instituições governamentais, com o fim de regular o uso e desenvolvimento da IA, rigorosamente (regulamentos juridicamente vinculativos) ou suavemente (diretrizes juridicamente não vinculativas);
**Auto-regulamentação/Auto-regulamentação Voluntária:** esta categoria foi definida para englobar documentos feitos por organizações privadas e outros órgãos que defendem uma forma de auto-regulamentação governada pela própria indústria da IA. Ela também abrange o auto-compromisso voluntário feito por organizações independentes (ONGs, Associações Profissionais, etc.);
**Recomendação:** esta categoria foi definida para englobar documentos que apenas sugerem possíveis formas de governança e princípios éticos que devem orientar as organizações que buscam usar, desenvolver ou regular as tecnologias de IA.
Quando analisamos a forma de regulamentação proposta pelos documentos de nossa amostra, **mais da metade (56%) são apenas recomendações** para diferentes stakeholders, enquanto que **24% dos documentos analisados foram enquadrados na categoria auto-regulamentação/auto-regulamentação voluntária**, e somente **20%** propuseram uma **forma de regulação administrada por um dado estado/país**.
Este tipo está relacionado com o escopo de impacto que motiva o documento. Com escopo de impacto, nos referimos aos riscos e benefícios relacionados ao uso da IA que motivam o tipo de regulamentação sugerida pelo documento. Para isso, **três categorias** foram definidas (estas categorias foram *definidas como mutuamente exclusivas*):
**Short-Termism:** esta categoria foi definida para englobar documentos nos quais o escopo do impacto e da preocupação se concentra principalmente em problemas de curto prazo, ou seja, problemas que estamos enfrentando com as atuais tecnologias de IA (e. g., discriminação algorítmica, opacidade algorítmica, privacidade, responsabilidade legal);  
**Long-Termism:** esta categoria foi definida para englobar documentos nos quais o escopo do impacto e da preocupação se concentra principalmente em problemas de longo prazo, ou seja, problemas que podemos vir a enfrentar com futuros sistemas de IA. Como tais tecnologias ainda não são uma realidade, tais riscos podem ser classificados como hipotéticos ou, na melhor das hipóteses, incertos (e. g, IA senciente, AGI desalinhada, IA super inteligente, riscos existenciais relacionados à IA);
**Short-Termism & Long-Termism:** esta categoria foi projetada para englobar documentos nos quais o escopo do impacto é tanto de curto como de longo prazo, ou seja, eles apresentam um escopo de "médio prazo" de preocupação. Estes documentos abordam questões relacionadas à categoria de curto prazo, ao mesmo tempo em que apontam os impactos de longo prazo de nossa atual adoção da IA (e. g., interferência da IA nos processos democráticos, armas autônomas, riscos existenciais, sustentabilidade ambiental, deslocamento de mão-de-obra, a necessidade de atualizar nossos sistemas educacionais).
Olhando para a totalidade de nossa amostra, vemos claramente que **curto (47%)** e **"médio prazo" (i.e., Short-Termism & Long-Termism = 52%)** prevalecem sobre **preocupações a longo prazo (2%)**. Quando filtramos nossa amostra por **escopo de impacto a tipo de instituição** nós vemos que **corporações privadas pensam sobre os impactos relacionados a IA mais a curto prazo(33%)**, enquanto que **instituições governamentais tendem a focar no médio prazo (28%)**, e **instituições acadêmicas (66%) e organizações sem fins lucrativos (33%) são as que mais levantam questões definidas como long-term** 
Este tipo está relacionado à força normativa do mecanismo de regulamentação proposto pelo documento. Para isso, foram definidas **duas categorias** (estas categorias *não foram definidas como mutuamente exclusivas*):
**Diretivas juridicamente não vinculativas:** estes documentos propõem uma abordagem que entrelaça princípios éticos com práticas recomendadas para empresas e outras entidades (i. e., soluções jurídicas não vinculativas);
**Regulamentos juridicamente vinculativos:** estes documentos propõem uma abordagem que se concentra na regulamentação de usos específicos da IA em regulamentos juridicamente vinculativos, como exigências e proibições obrigatórias.
A falta de convergência para uma forma de regulamentação mais **"baseada no governo"** se reflete na força normativa dos documentos analisados, onde a **vasta maioria (98%) serve apenas como "leis brandas"**, ou seja, tais diretrizes não implicam qualquer forma de obrigação legal, enquanto **somente 4,5% apresentam formas mais rígidas de regulamentação.** Uma vez que apenas as instituições governamentais podem apresentar normas legalmente vinculativas (outras formas de instituições não possuem tal poder), e **instituições governamentais produziram apenas 24% de nossa amostra**, alguns podem argumentar que esse desequilíbrio reside nesse fato.
Entretanto, **filtrando** somente os documentos produzidos por **instituições governamentais**, a **desproporção não desaparece**, com **somente 18,7% dos documentos propondo formas de regulamentação juridicamente vinculativas**. Os países que parecem estar à frente desta, ainda fraca, tendência são **Canadá**, **Alemanha**, e o **Reino Unido**, com a Austrália, Noruega, e os EUA vindo logo atrás.
Como uma contribuição final, para cada documento de nossa amostra, foi escrito um breve resumo, permitindo ao leitor ter uma rápida visão do conteúdo de cada documento. Cada documento também foi vinculado a sua URL. Em nosso [Power BI dashboard](https://www.airespucrs.org/worldwide-ai-ethics), também é possível encontrar os websites da instituição de origem, e outros documentos importantes anexados e citados na publicação original.
Aqui podemos ver casos de *"divergência na definição de princípios"*, ou seja, **formas divergentes de definição de princípios éticos**. A título de exemplo, vejamos nosso princípio mais citado: **Transparência/Explicabilidade/Auditoria.**
Ao examinar a definição proposta em "[ARCC: An Ethical Framework for Artificial Intelligence](https://www.tisi.org/13747):" 
*"Promover a transparência algorítmica e a auditoria algorítmica, para alcançar sistemas de IA compreensíveis e explicáveis. Explicar as decisões assistidas/feitas por sistemas de IA, quando apropriado. Assegurar o direito dos indivíduos de conhecer e fornecer aos usuários informações suficientes sobre o propósito, função, limitação e impacto do sistema de IA."* 
Em comparação com a definição fornecida em "[A practical guide to Responsible Artificial Intelligence (AI)](https://www.pwc.com/gx/en/issues/data-and-analytics/artificial-intelligence/what-is-responsible-ai/responsible-ai-practical-guide.pdf):"
*"Para incutir confiança nos sistemas de IA, as pessoas devem ser habilitadas a olhar sob o capô de seus modelos subjacentes, explorar os dados usados para treiná-los, expor o raciocínio por trás de cada decisão e fornecer prontamente explicações coerentes a todos stakeholders. Estas explicações devem ser adaptadas às diferentes partes interessadas, incluindo reguladores, cientistas de dados, patrocinadores de negócios e consumidores finais."*
Ambas as definições são similares, porém *" the AI-devil is in the details."* Somente a **primeira definição implica o conceito de auditoria,** o que significa (em algumas interpretações) uma revisão do sistema em questão por terceiros. Além disso, enquanto o primeiro documento menciona que *"é preciso explicar",* *"garantir o certo",* e *"prover informações suficientes para as pessoas",* impondo claramente a ideia de um *"dever de explicar"* (**sem especificar quem deve explicar**), junto com o *"direito de saber"*,  o segundo documento menciona que pessoas *"tem de ser capaz de olhar além"* (**também sem especificar quem deve poder olhar além**), sem trazer a ideia de direito ou dever. Ao mesmo tempo, **apenas o segundo propõe que este conhecimento seja adaptado e acessível a diferentes tipos de stakeholders**, já que uma explicação adequada para um engenheiro de aprendizagem de máquina pode não ser adequada para um consumidor leigo.
Tendo em mente que o conceito de **transparência/interpretabilidade é uma ideia/conceito fundamental na Ética da IA** (especialmente em pesquisas de aprendizagem de máquina), estando ainda sujeito a divergências em sua interpretação/aplicação, que tipos de diferenças podem ocorrer quando olhamos para *"princípios não tão bem definidos"*, como **centrado no ser-humano**
Em "Data, Responsibly (Vol. 1) Mirror, Mirror", (Khan & Stoyanovich, [2020](https://dataresponsibly.github.io/comics/)), encontramos a seguinte recomentação:
*"Talvez o que precisamos, ao invés disso, seja fundamentar o projeto de sistemas de IA nas pessoas. Usando os dados das pessoas, coletados e implantados com uma metodologia equitativa, conforme determinado pelas pessoas, para criar tecnologia que seja benéfica para as pessoas."*
Enquanto que em "[Everyday Ethics for Artificial Intelligence](https://www.ibm.com/watson/assets/duo/pdf/everydayethics.pdf)." a seguinte norma foi sugerida:
*"AI deve ser projetado para se alinhar com as normas e valores de seu grupo de usuários em mente."*
O primeiro documento menciona ideias como *"o uso de uma metodologia equitativa"* e *"tecnologia que seja benéfica para as pessoas"*. Esta ideia de *"pessoas"* parece se referir a um grande e diversificado  grupo (talvez "todas as pessoas"). Enquanto isso, o segundo declara especificamente *"seu grupo de usuários em mente"*, o que poderia significar *"um pequeno e seleto grupo de pessoas"*, se é isso que os designers têm em mente como *"seus usuários"*.
Muitas outras diferenças podem ser encontradas em nossa amostra, por exemplo: 
“[Tieto’s AI ethics guidelines](https://www.tietoevry.com/en/newsroom/all-news-and-releases/press-releases/2018/10/tieto-strengthens-commitment-to-ethical-use-of-ai/)” assume uma outra postura de explicabilidade, dizendo que seus sistemas *"podem ser explicados e se explicam"*, colocando parte da responsabilidade de explicabilidade no próprio sistema **AI**, tornando-o um "stakeholder" na cadeia de responsabilidade (uma abordagem curiosa); 
“[The Toronto Declaration](https://www.torontodeclaration.org/declaration-text/english/)” dá uma definição extensa e não exaustiva do que *"discriminação"* significa segundo as leis internacionais, enquanto a maioria dos outros documentos resumem-se a apenas citar o conceito, deixando em aberto a interpretação dos tipos de *"discriminação que são permitids"*;
Em “[Artificial Intelligence and Machine Learning: Policy Paper](https://www.internetsociety.org/resources/doc/2017/artificial-intelligence-and-machine-learning-policy-paper/)”, a justiça está relacionada à ideia de *"a IA proporciona oportunidades sócio-econômicas para todos"* (**benefícios**), e em “[Trustworthy AI in Aotearoa: AI Principles](https://aiforum.org.nz/wp-content/uploads/2020/03/Trustworthy-AI-in-Aotearoa-March-2020.pdf)” justiça é definida como *"sistemas de IA não prejudicam injustamente"* (**impactos**), o que podemos relacionar com a **diferença entre certas noções de justiça algorítmica** (paridade preditiva versus probabilidades equalizadas); 
Enquanto alguns documentos (e.g., “[Telefónica´s Approach to the Responsible Use of AI](https://www.telefonica.com/en/wp-content/uploads/sites/5/2021/08/ia-responsible-governance.pdf)”) afirmam como a privacidade e a segurança são essenciais para o desenvolvimento de sistemas de IA, apenas alguns (e.g., “[Big Data, Artificial Intelligence, Machine Learning, and Data Protection](https://ico.org.uk/media/for-organisations/documents/2013559/big-data-ai-ml-and-data-protection.pdf)”) especificam o que *“bons critérios de privacidade”* são (e.g., **data minimization**).
Enquanto a maioria dos documentos interpreta Responsabilidade/Responsabilização como *"desenvolvedores sendo responsáveis por seus projetos "*(e.g., “[Declaration of Ethical Principles for AI in Latin America](https://ia-latam.com/etica-ia-latam/)”), **alguns documentos também colocam esta responsabilidade sobre os usuários**, e até mesmo os “próprios algoritmos” (e.g., “[The Ethics of Code: Developing AI for Business with Five Core Principles](https://www.sage.com/~/media/group/files/business-builders/business-builders-ethics-of-code.pdf?la=en)”).
Além das comparações mencionadas acima, muitas outras podem ser feitas utilizando nosso conjunto de dados (disponível para download no final desta página).
Além disso, quando examinamos apenas a categoria de Ciências da Computação, **" Visão Computacional e Reconhecimento de Padrões", "Aprendizagem de Máquina",** e **"Computação e Linguagem"** são os tipos de subcategorias mais apresentados. **Note que todas estas são áreas onde a aprendizagem de máquina se encontra estabelecida como seu paradigma atual.** 
Além do número de publicações sendo produzidos, nunca tivemos mais capital sendo investido em empresas e startups, seja por governos ou fundos de *venture capital*. **( mais de 90 bilhões de USD$ em 2021 só nos EUA)**, e patentes relacionadas à IA sendo registradas (Zhang et al., [2022](https://aiindex.stanford.edu/report/)). Esta rápida expansão do campo/indústria da IA também veio com outro boom, o *"boom da ética da IA",* onde uma exigência nunca antes vista de regulamentação e orientação normativa de tais tecnologias foi manifestada. Baseando-se no trabalho realizado por outros meta-analistas do campo, **este estudo apresenta uma revisão sistemática de 200 documentos relacionados à ética e governança da IA.** Nós apresentamos uma coleção de **tipologias usadas para classificar nossa amostra**, tudo condensado em uma ferramenta on-line **interativa e de acesso aberto**, juntamente com uma análise crítica daquilo que *"está sendo dito"* e *"quem o está dizendo"* em nosso panorama global da ética da IA.