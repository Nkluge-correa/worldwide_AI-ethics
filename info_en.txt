Since the end of our last *“AI winter,”* 1987 – 1993, AI research and its industry have seen massive growth, either in developed technologies, investment, media attention, or new tasks that autonomous systems are nowadays able to perform. By looking at the history of submissions in ArXiv ([between 2009 and 2021](https://arxiv.org/about/reports/submission_category_by_year)), an open-access repository of electronic preprints and postprints, starting from 2018, **Computer Science-related papers have been the most common sort of submitted material.** 
Corrêa, N.K., Galvão, C., Pinto, E.S., William, J., Del Pino, C., Barbosa, C., Massmann, D.F., Mambrini, R., Galvão, L., Terem, E. (2022). Worldwide AI Ethics: a review of 200 guidelines and recommendations for AI governance. *AI Robotics Ethics Society (PUCRS).* [https://doi.org/10.48550/arXiv.2206.11922](https://doi.org/10.48550/arXiv.2206.11922).
Looking at the distribution among world regions (aggregated by continent), we see that the bulk of produced documents come from **Europe, North America, and Asia**, while regions like **South America, Africa and Oceania represent less than 4,5% of our entire sample size**. If it was not for the **significant participation of Intergovernmental Organizations,** like NATO, UN, UNESCO, **that represent 6% of our sample size (13 documents),** other world regions/countries would be even more underrepresented.
77% of our total sample size are represented by 13 countries, **United States of America, United Kingdom, Germany, Canada, China, Japan, France, Finland, Netherlands, Switzerland, Belgium, Brazil, and South Korea,** while a myriad of **24 countries (12,5%) represents the remainder of our sample**, together with Intergovernmental organizations, like the EU (9 = 4,5%) and the UN (6 = 3%).
When removing documents with unspecified authors, we counted a total of **561 male authors (66,6%)** and **281 female authors (33,3%).** The dominance of male authors over female authors is **a trend that can be found in practically every world region and country, regardless of institution type.**
The gender of the authors was determined by searching their names and profile pictures on different types of platforms (e.g., *LinkedIn, Researchgate,  University websites,  personal websites, etc.*)  through search engines. 
With respect to the year of publication of the documents from our sample, one can see that **the majority of documents (129 = 64,5%) Were published between the years 2017 and 2019.** What we may call the *"AI ethics boom"* would be the **significant production of documents in the year 2018**, which represents **30,5% (61)** of our entire sample.
**Note:** documents with **unspecified** dates of publication **(27 = 13,5%)** are also quite **prevalent** in our sample.
The **top five principles** advocated in the documents of our sample are similar to the results shown by Jobin et al. ([2019](https://www.nature.com/articles/s42256-019-0088-2)) and Hagendorff ([2020](https://link.springer.com/article/10.1007/s11023-020-09517-8)), **with the addition of Reliability/Safety/Security/Trustworthiness (78%)**, which was also cited as top five in Fjeld et al. ([2020](https://dash.harvard.edu/handle/1/42160420)) meta analysis **(80%)**. Since each document presents its own passage about each principle, if there were, for example, 134 documents that upheld privacy, we collected 134 different definitions/recommendations involving this principle. All being accessible in our [Power BI dashboard](https://en.airespucrs.org/worldwide-ai-ethics).
Looking at **principle distribution filtered by continent**, the **top five principles remain the same** in both **North America** and **Europe**, but the **Asian continent** introduces the principle of **Beneficence/Non-Maleficence as is 5th (74%) most cited principle**, putting Accountability/Liability in 6th place (70%). **Filtering our results by country**, we see **no change in the top five principles** when comparing **EUA** and the **UK**. However, looking **under the top five principles**, we begging to see **differences**, like **Freedom/Autonomy/Democratic Values/Technological Sovereignty (38%)** and **Beneficence/Non-Maleficence (34,4%)** being the **6th** and **7th most cited principles in the EUA**, and **Open source/Fair Competition/Cooperation (45,8%)** and **Diversity/Inclusion/Pluralism/Accessibility (41,6%)** being **6th** and **7th most cited principles in the UK**.
When examining **principle distribution filtered by institution type**, we also can find many insights. For example, looking at our total sample, we see that the **main preoccupation of governmental institutions** (world-wide) is the need for **transparent systems (89,5%)**, **private corporations** maily advocate for **Reliability (87,5%)**, and **CSO/NGOs** primarily defend the principle of **fairness (88,2%)**. 
Except for institutions like **IBM (5)**, **Microsoft (4)**, and **UNESCO (3)**, **most other institutions do not have more than two published documents**. We can also see that the **bulk of our sample was produced by governmental institutions and private corporations (48%)**,  followed by **CSO/NGO (17%)**, **non-profit organizations (16%)**, and **academic institutions (12,5%)**. 
However, **this trend only follows if we look at the totality of our sample size**. If we look at documents produced by **continents**, for example, in **North America (69), private corporations (24 = 34,7%) and nonprofit organizations (18 = 26%) produced most documents**, followed by governmental institutions (12 = 17,4%). Meanwhile, when we look at **Europe**, the **global trend is restored**. 
An in-depth analysis **segmented by countries** shows that the **engagement** of certain types of **AI stakeholders** (i.e., institution types) **differs between countries.** For example, in **China (11),  the majority of documents are produced by academic institutions (5 = 45,4%), while in Germany (20), most documents in our sample were produced by private corporations (6 = 30%), and CSO/NGO (4 = 20%).** Other insights can be found in our [Power BI dashboard](https://en.airespucrs.org/worldwide-ai-ethics).
This type relates to the nature/content of the document, and **three categories** were defined (these categories were *not defined as mutually exclusive*):
**Descriptive:** descriptive documents take the effort of presenting factual definitions related to AI technologies. These definitions serve to contextualize "what we mean" when we talk about AI, and how the vocabulary utilized in this field can be understood;
**Normative:** normative documents present norms, ethical principles, recommendations, and imperative affirmations about what such technologies should, or should not, be used/developed for;
**Practical:** practical documents present development tools to implement ethical principles and norms, be they qualitative (e.g., Self-assessment surveys) or quantitative (e.g., Debiasing Algorithms for ML models).
The majority of our sample is comprised of **normative samples (96%)**, which a **third of the time also presents descriptive contents (55,5%)**, and more **rarely**, **practical** implementations **54 (27%)**.
To create an "**overall definition**" of each principle/group of principles, we utilize a **[text mining](https://en.wikipedia.org/wiki/Text_mining)** technique called **[n-gram analysis](https://en.wikipedia.org/wiki/N-gram)**, were we counted the successive repetition of words (and groups of words) in every principle found in the documents of our sample. Thus, the bellow definitions were created to contemplate the recurring themes we encountered. *Below we also present count charts for four-grams of each principle.*
**Accountability/Liability:** accountability refers to the idea that developers and deployers of AI technologies should be compliant with regulatory bodies, also meaning that such actors should be accountable for their actions and the impacts caused by their technologies;
**Beneficence/Non-Maleficence:** beneficence and non-maleficence are concepts that come from bioethics and medical ethics. In AI ethics, these principles state that human welfare (and harm aversion) should be the goal of AI-empowered technologies. Sometimes, this principle is also tied to the idea of Sustainability, stating that AI should be beneficial not only to human civilization but to our natural environment and other living creatures; 
**Children & Adolescents Rights:** the idea that the rights of children and adolescents must be respected by AI technologies. AI stakeholders should safeguard, respect, and be aware of the fragilities associated with young people;
**Dignity/Human Rights:** this principle is based on the idea that all individuals deserve proper treatment and respect. In AI ethics, the respect for human dignity is often tied to human rights (i.e., Universal Declaration of Human Rights); 
**Diversity/Inclusion/Pluralism/Accessibility:** this set of principles advocates the idea that the development and use of AI technologies should be done in an inclusive and accessible way, respecting the different ways that the human entity may come to express itself (gender, ethnicity, race, sexual orientation, disabilities, etc.). This meta-principle is strongly tied to another set of principles: Justice/Equity/Fairness/Non-discrimination; 
**Freedom/Autonomy/Democratic Values/Technological Sovereignty:** this set of principles advocates the idea that the autonomy of human decision-making must be preserved during human-AI interactions, whether that choice is individual, or the freedom to choose together, such as the inviolability of democratic rights and values, also being linked to technological self-sufficiency of Nations/States;
**Human Formation/Education:** such principles defend the idea that human formation and education must be prioritized in our technological advances. AI technologies require a considerable level of expertise to be produced and operated, and such knowledge should be accessible to all. This principle seems to be strongly tied to Labor Rights. The vast majority of documents concerned with workers and the work-life point to the need for re-educating and re-skilling the workforce as a mitigation strategy against technological unemployment;
**Human-Centeredness/Alignment:** such principles advocate the idea that AI systems should be centered on and aligned with human values. AI technologies should be tailored to align with our values (e.g., value-sensitive design). This principle is also used as a "catch-all" category, many times being defined as a collection of "principles that are valued by humans" (e.g., freedom, privacy, non-discrimination, etc.);
**Intellectual Property:** this principle seeks to ground the property rights over AI products and/or processes of knowledge generated by individuals, whether tangible or intangible;
**Justice/Equity/Fairness/Non-discrimination:** this set of principles upholds the idea of non-discrimination and bias mitigation (discriminatory algorithmic biases AI systems can be subject to). It defends the idea that, regardless of the different sensitive attributes that may characterize an individual, all should be treated "fairly";
**Labor Rights:** labor rights are legal and human rights related to the labor relations between workers and employers. In AI ethics, this principle emphasizes that workers' rights should be preserved regardless of whether labor relations are being mediated/augmented by AI technologies or not. One of the main preoccupations pointed out when this principle is presented is the mitigation of technological unemployment (e.g., through Human Formation/Education);
**Open source/Fair Competition/Cooperation:** this set of principles advocates different means by which joint actions can be established and cultivated between AI stakeholders to achieve common goals. It also advocates for the free and open exchange of valuable AI assets (e.g., data, knowledge, patent rights, human resources) to mitigate possible AI/technology monopolies;
**Privacy:** the idea of privacy can be defined as the individual's right to "expose oneself voluntarily, and to the extent desired, to the world." In AI ethics, this principle upholds the right of a person to control the exposure and availability of personal information when mined as training data for AI systems. This principle is also related to concepts such as data minimization, anonymity, informed consent, and other data protection-related concepts; 
**Reliability/Safety/Security/Trustworthiness:** this set of principles upholds the idea that AI technologies should be reliable, in the sense that their use can be verifiably attested as safe and robust, promoting user trust and better acceptance of AI technologies;
**Sustainability:** this principle can be understood as a form of "intergenerational justice," where the well-being of future generations must also be counted during AI development. In AI ethics, sustainability refers to the idea that the development of AI technologies should be carried out with an awareness of their long-term implications, such as environmental costs and non-human life preservation/well-being;
**Transparency/Explainability/Auditability:** this set of principles supports the idea that the use and development of AI technologies should be done transparently for all interested stakeholders. Transparency can be related to "the transparency of an organization" or "the transparency of an algorithm." This set of principles is also related to the idea that such information should be understandable to nonexperts, and when necessary, subject to being audited;
**Truthfulness:** this principle upholds the idea that AI technologies must provide truthful information. It is also related to the idea that people should not be deceived when interacting with AI systems. This principle is strongly related to the mitigation of automated means of disinformation.
This type relates to the form of regulation that the document proposes. For this, **three categories** were defined (these categories were *defined as mutually exclusive*):
**Government-Regulation:** this category is designed to encompass documents made by governmental institutions to regulate the use and development of AI, strictly (Legally binding horizontal regulations) or softly (Legally non-binding guidelines);
**Self-Regulation/Voluntary Self-Commitment:** this category is designed to encompass documents made by private organizations and other bodies that defend a form of Self-Regulation governed by the AI industry itself. It also encompasses voluntary self-commitment made by independent organizations (NGOs, Professional Associations, etc.);
**Recommendation:** this category is designed to encompass documents that only suggest possible forms of governance and ethical principles that should guide organizations seeking to use, develop, or regulate AI technologies.
When we look at the form of regulation proposed by the documents of our sample, **more than half (56%) are only recommendations** to different AI stakeholders, while **24% present self-regulatory/voluntary self-commitment style guidelines**, and only **20%** propose a **form of regulation administered by a given state/country**.
This type relates to the impact scope that motivates the document. With impact scope, we mean the related risks and benefits regarding the use of AI that motivate the type of regulation suggested by the document. For this, **three categories** were defined (these categories were *defined as mutually exclusive*):
**Short-Termism:** this category is designed to encompass documents in which the scope of impact and preoccupation focus mainly on short-term problems, i.e., problems we are facing with current AI technologies (e.g., algorithmic discrimination, algorithmic opacity, privacy, legal accountability);  
**Long-Termism:** this category is designed to encompass documents in which the scope of impact and preoccupation focus mainly on long-term problems, i.e., problems we may come to face with future AI systems. Since such technologies are not yet a reality, such risks can be classified as hypothetical or, at best, uncertain (e.g., sentient AI, misaligned AGI, super intelligent AI, AI-related existential risks);
**Short-Termism & Long-Termism:** this category is designed to encompass documents in which the scope of impact is both short and long-term, i.e., they present a "mid-term" scope of preoccupation. These documents address issues related to the Short-Termism category, while also pointing out the long-term impacts of our current AI adoption (e.g., AI interfering in democratic processes, autonomous weapons, existential risks, environmental sustainability, labor displacement, the need for updating our educational systems).
Looking at the totality of our sample size, we see clearly that **short-term (47%)** and **"mid-term" (i.e., Short-Termism & Long-Termism = 52%)** prevail over more **long-term preoccupations (2%)**. When we filter our sample by **impact scope and institution type**, it seems to us that **private corporations think more about the short-term (33%)**, **governmental institutions about the short/long-term (28%)**, and **academic (66%) and non-profit organizations (33%) with the long-term** impacts of AI technologies.
This type relates to the normative strength of the regulation mechanism proposed by the document. For this, **two categories** were defined (these categories were *not defined as mutually exclusive*):
**Legally non-binding guidelines:** these documents propose an approach that intertwines AI principles with recommended practices for companies and other entities (i.e., soft law solutions);
**Legally binding horizontal regulations:** these documents propose an approach that focuses on regulating specific uses of AI on legally binding horizontal regulations, like mandatory requirements and prohibitions.
This lack of convergence to a more **"government based"** form of regulation is reflected in the normative strength of these documents, where the **vast majority (98%) only serve as "soft laws,"** i.e., guidelines that do not entail any form of a legal obligation, while **only 4,5% present more strict forms of regulation.** Since only governmental institutions can come up with legally binding norms (other forms of institutions lack this power), and **governmental institutions produced only 24% of our sample**, some may argue that this imbalance lies in this fact.
However, **filtering** only the documents produced by **governmental institutions**, the **disproportion does not go away**, with **only 18,7% of documents proposing legally binding forms of regulation**. The countries that seem to be spearheading this still weak trend are **Canada**, **Germany**, and the **United Kingdom**, with Australia, Norway, and the USA coming right behind.
As a final contribution, for every document on our sample, a brief abstract was written, allowing the reader to have a quick review of the contents of each document. Every document was also linked to its URL. In our [Power BI dashboard](https://en.airespucrs.org/worldwide-ai-ethics), one can also find the websites of the parent institution, and other important attached documents cited in the original publication.
Here we can see cases of *"principle definition divergence,"* i.e., **divergent forms of defining ethical principles**. As an example, let us look at our most cited principle: **Transparency/Explainability/Auditability.**
When examining the definition proposed in "[ARCC: An Ethical Framework for Artificial Intelligence](https://www.tisi.org/13747):" 
*"Promote algorithmic transparency and algorithmic audit, to achieve understandable and explainable AI systems. Explain the decisions assisted/made by AI systems when appropriate. Ensure individuals’ right to know, and provide users with sufficient information concerning the AI system’s purpose, function, limitation, and impact."* 
In comparison with the one provided in "[A practical guide to Responsible Artificial Intelligence (AI)](https://www.pwc.com/gx/en/issues/data-and-analytics/artificial-intelligence/what-is-responsible-ai/responsible-ai-practical-guide.pdf):"
*"To instill trust in AI systems, people must be enabled to look under the hood at their underlying models, explore the data used to train them, expose the reasoning behind each decision, and provide coherent explanations to all stakeholders promptly. These explanations should be tailored to the different stakeholders, including regulators, data scientists, business sponsors, and end consumers."*
Both definitions seem similar, but the *"AI-devil is in the details."* Only the **first definition entails the concept of auditing,** which means (in some interpretations) a third-party review of the system in question. Also, while the first document mentions that *"one must explain,"* *"ensure the right,"* and *"provide enough information for people,"* clearly imposing the idea of a *"duty to explain"* (**without specifying who should explain**), together with the *"right to know"*,  the second document says that people have *"to be able to look under the hood"* (**also without specifying who should be able to look**), without bringing the idea of right or duty. Also, **only the second one proposes that this knowledge should be tailored and accessible to different types of stakeholders**, since an explanation fit for a machine learning engineer may not be fit for a consumer of an AI-empowered product.
Keeping in mind that the concept of **transparency/interpretability is a well-fundamental idea/concept in AI** (especially machine learning research), being still subject to divergence in its interpretation/application, what kinds of differences may occur when we look at *"not so well defined"* principles, like **human-centeredness.**
In "Data, Responsibly (Vol. 1) Mirror, Mirror," (Khan & Stoyanovich, [2020](https://dataresponsibly.github.io/comics/)), we find the following recommendation:
*"Maybe what we need instead is to ground the design of AI systems in people. Using the data of the people, collected and deployed with an equitable methodology as determined by the people, to create technology that is beneficial for the people."*
While in "[Everyday Ethics for Artificial Intelligence](https://www.ibm.com/watson/assets/duo/pdf/everydayethics.pdf)",the following norm is suggested:
*"AI should be designed to align with the norms and values of your user group in mind."*
The first document mentions ideas like *"the use of an equitable methodology"* and *"technology that is beneficial for the people."* This idea of *"people"* seems to refer to a large and diverse group (perhaps "all people"). Meanwhile, the second specifically states *"your user group in mind,"*  which could mean *"a small and select group of people,"* if that is what the designers have in mind as *"their users."*
Many other differences can be found in our sample, for example:
“[Tieto’s AI ethics guidelines](https://www.tietoevry.com/en/newsroom/all-news-and-releases/press-releases/2018/10/tieto-strengthens-commitment-to-ethical-use-of-ai/)” takes a different take on explainability, saying its systems *“can be explained and explain itself”*, potting some of the responsibility of explainability in the **AI system itself**, making it a “stakeholder” in the accountability chain (a curious approach); 
“[The Toronto Declaration](https://www.torontodeclaration.org/declaration-text/english/)” gives an extensive and nonexhaustive definition of what *“discrimination”* means under international laws, while most other documents resume themselves in only citing the concept, leaving open to interpretation the types of *“discrimination that is permissible”*;
In “[Artificial Intelligence and Machine Learning: Policy Paper](https://www.internetsociety.org/resources/doc/2017/artificial-intelligence-and-machine-learning-policy-paper/)”, fairness is related to the idea of *“AI provides socio-economic opportunities for all”* (**benefits**), in “[Trustworthy AI in Aotearoa: AI Principles](https://aiforum.org.nz/wp-content/uploads/2020/03/Trustworthy-AI-in-Aotearoa-March-2020.pdf)” fairness is also defined as *“AI systems do not unjustly harm”* (**impacts**), which we can relate to the **difference between certain notions of algorithmic fairness** (predictive parity vs equalized odds); 
While some documents (e.g., “[Telefónica´s Approach to the Responsible Use of AI](https://www.telefonica.com/en/wp-content/uploads/sites/5/2021/08/ia-responsible-governance.pdf)”) state how privacy and security are essential for AI systems developments, only some (e.g., “[Big Data, Artificial Intelligence, Machine Learning, and Data Protection](https://ico.org.uk/media/for-organisations/documents/2013559/big-data-ai-ml-and-data-protection.pdf)”) specify what *“good privacy criteria”* are (e.g., **data minimization**).
While most documents interpret accountability/liability as *“developers being responsible for their projects”* (e.g., “[Declaration of Ethical Principles for AI in Latin America](https://ia-latam.com/etica-ia-latam/)”), **some documents also put this responsibility on users**, and even algorithms “themselves” (e.g., “[The Ethics of Code: Developing AI for Business with Five Core Principles](https://www.sage.com/~/media/group/files/business-builders/business-builders-ethics-of-code.pdf?la=en)”).
Besides the ones mentioned above, many other forms of comparisons can be made using our dataset (available for download at the bottom of this page).
Also, when we examine the category of Computer Science alone, **"Computer Vision and Pattern Recognition," "Machine Learning,"** and **"Computation and Language"** are the most submitted types of sub-categories. **Note that all of these are areas where Machine Learning is notably established as its current paradigm.** 
Besides the number of papers being produced, we have never had more capital being invested in AI-related companies and startups, either by governments or Venture Capital firms **(more than 90 billion USD$ in 2021 in the US alone)**, and AI-related patents being registered (Zhang et al., [2022](https://aiindex.stanford.edu/report/)). This rapid expansion of the AI field/industry also came with another boom, the *"AI Ethics boom,"* where a never before seen demand for regulation and normative guidance of these technologies has been put forward. Drawing on the work done by past meta-analysts in the field, **this study presents a systematic review of 200 documents related to AI ethics and governance.** We present a collection of **typologies used to classify our sample**, all condensed in an **interactive and open-access online tool**, coupled with a critical analysis of *"what is being said"* and *"who is saying it"* in our AI ethics global landscape.